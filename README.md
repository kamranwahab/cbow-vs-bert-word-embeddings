# ğŸ§  How Do Machines Understand Words?

## CBOW vs BERT â€” Static vs Contextual Word Embeddings

---

<p align="center">
  <img src="https://img.shields.io/badge/NLP-Word%20Embeddings-blue" />
  <img src="https://img.shields.io/badge/Model-CBOW%20(from%20scratch)-orange" />
  <img src="https://img.shields.io/badge/Model-BERT-green" />
  <img src="https://img.shields.io/badge/Status-Completed-success" />
</p>

---

## ğŸ“Œ Project Overview

* This project explores **how machines learn word meaning**
* Implements **CBOW from scratch** (no deep-learning framework shortcuts)
* Compares **static embeddings (CBOW)** with **contextual embeddings (BERT)**
* Strictly follows an academic NLP assignment specification
* Presented as a **clean, reproducible Jupyter notebook**

---

## âš¡ Why This Project Matters (Recruiter Perspective)

* Most NLP projects **only use pretrained embeddings**
* This project:

  * builds embeddings **from first principles**
  * implements **manual forward & backward propagation**
  * validates ideas through **experiments and visualizations**
* Demonstrates:

  * strong NLP fundamentals
  * understanding of model internals
  * ability to explain results clearly

---

## ğŸ§© Core Question Addressed

* â“ Does a word always have the same meaning?
* ğŸ§ª Experiment:

  * Compare **CBOW** vs **BERT**
* ğŸ“Œ Example:

  * *â€œI deposited money in the bankâ€*
  * *â€œThey sat near the river bankâ€*

**Expected behavior**

* CBOW â†’ same embedding
* BERT â†’ different embeddings

---

## ğŸ—ï¸ What Was Built

### ğŸ”¹ CBOW Model (From Scratch)

* Implemented using **NumPy only**
* No PyTorch / TensorFlow for CBOW
* Explicit implementation of:

  * one-hot encoding
  * context vector averaging
  * matrix multiplication
  * softmax activation
  * cross-entropy loss
  * gradient descent updates
* Confirms **conceptual mastery**, not library dependency

---

## ğŸ“š Dataset & Preprocessing

* Dataset:

  * **Tiny Shakespeare Corpus**
* Preprocessing steps:

  * lowercasing
  * whitespace tokenization
  * punctuation removal
* Training setup:

  * ordered 80 / 20 train-test split
  * context window = **2 left + 2 right**

---

## ğŸ“‰ CBOW Training Behavior

* Trained for multiple epochs
* Observed:

  * consistent decrease in training loss
  * stable convergence behavior
* Confirms:

  * correct forward pass
  * correct backpropagation
  * correct parameter updates

---

## ğŸ“Š What CBOW Learns

* Captures **co-occurrence-based semantics**
* Similar words move closer in embedding space
* Limitations:

  * one word â†’ one fixed vector
  * no context awareness
  * cannot handle polysemy

---

## ğŸ¨ Embedding Visualization

* Dimensionality reduction using **PCA**
* Observations:

  * semantically related words form clusters
  * rare or ambiguous words appear noisier
* Visualization makes embeddings:

  * interpretable
  * explainable

---

## ğŸ¤– BERT Contextual Embeddings

* Model used:

  * `bert-base-uncased`
* Extracted:

  * token-level embeddings from last hidden layer
* Key behavior:

  * same word â†’ different vectors in different contexts
* Demonstrates:

  * context-aware representation
  * strength of transformer architectures

---

## âš”ï¸ CBOW vs BERT â€” Experimental Comparison

| Feature           | CBOW         | BERT                |
| ----------------- | ------------ | ------------------- |
| Embedding type    | Static       | Contextual          |
| Context awareness | âŒ            | âœ…                   |
| Polysemy handling | âŒ            | âœ…                   |
| Training data     | Small corpus | Massive pretraining |
| Real-world NLP    | Limited      | State-of-the-art    |

---

## ğŸ“Š Key Experimental Findings

* CBOW:

  * loss decreases over epochs
  * embeddings capture basic similarity
  * fails on multi-meaning words
* BERT:

  * embeddings vary with context
  * successfully separates different word senses
* Conclusion:

  * static embeddings â‰  sufficient for real NLP tasks

---

## â–¶ï¸ Run the Project (Google Colab)

* Open the notebook directly in Colab:

```
(https://colab.research.google.com/drive/1vfnWeomQRHlnkY_hM8kYnvetBZPIclUu)
```


## ğŸ§  Skills Demonstrated

* NLP fundamentals
* Neural network training from scratch
* Vector similarity analysis
* Embedding visualization
* Transformer-based contextual embeddings
* Experimental reasoning & interpretation

---

## ğŸ Final Takeaway

* **CBOW** explains *how* word meaning emerges from co-occurrence
* **BERT** explains *why* context is essential
* This project connects:

  * classical NLP theory
  * modern transformer practice

---

Just tell me.
