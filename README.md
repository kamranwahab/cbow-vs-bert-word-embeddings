# cbow-vs-bert-word-embeddings
A hands-on NLP project implementing CBOW from scratch and comparing it with contextual BERT embeddings to understand how modern language models represent meaning.

ğŸ“Œ Project Highlights

âœ” Implemented CBOW (Continuous Bag of Words) from scratch using NumPy
âœ” Explicit forward & backward propagation (no deep-learning framework shortcuts)
âœ” Trained on Tiny Shakespeare corpus (as per academic specification)
âœ” Visualized learned embeddings using PCA
âœ” Extracted contextual embeddings from BERT
âœ” Demonstrated polysemy handling (static vs contextual meaning)
âœ” Clear experimental comparison: CBOW vs BERT

This project shows strong fundamentals in NLP, linear algebra, and model internals, not just library usage.

ğŸ¯ Objectives

Learn how static word embeddings are trained using CBOW

Understand gradient-based training of embedding models

Explore contextual embeddings using Transformer models

Experimentally compare classical vs modern NLP approaches

ğŸ“‚ Dataset

Tiny Shakespeare Corpus

Preprocessing:

Lowercasing

Whitespace tokenization

Punctuation removal

Train/Test split: 80 / 20

Why Tiny Shakespeare?
It is small enough to train CBOW from scratch, yet rich enough to learn meaningful word co-occurrences.

ğŸ§  Model Overview
ğŸ”¹ CBOW (From Scratch)

Input: Average of context word one-hot vectors

Context window: 2 (2 left + 2 right)

Hidden layer: W1 (V Ã— D) â†’ word embeddings

Output layer: W2 (D Ã— V)

Loss: Cross-entropy

Optimization: Gradient Descent

Implemented manually, step by step

ğŸ”¹ BERT

Model: bert-base-uncased

Extracted embeddings from last hidden layer

Used to demonstrate context-aware representations

ğŸ“Š Experiments & Results
ğŸ” CBOW Training

Loss decreases consistently across epochs

Confirms correct forward/backward implementation

Learned embeddings capture basic semantic similarity

ğŸ¨ Embedding Visualization

PCA shows clustering of semantically related words

Some noise due to limited corpus size (expected)

ğŸ§ª Polysemy Experiment (â€œbankâ€)
Model	Observation
CBOW	Same vector regardless of context
BERT	Different vectors for different meanings

âœ” Clearly demonstrates static vs contextual embeddings

âš”ï¸ CBOW vs BERT â€” Quick Comparison
Feature	CBOW	BERT
Embedding Type	Static	Contextual
Polysemy Handling	âŒ	âœ…
Training Data	Small corpus	Massive pretraining
Interpretability	High	Medium
Real-world NLP	Limited	State-of-the-art
ğŸ› ï¸ Tech Stack
Python
NumPy
Matplotlib
Scikit-learn
Hugging Face Transformers

PyTorch (for BERT inference only)
